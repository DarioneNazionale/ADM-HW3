{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving web pages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this lines create a new directory, Movies, in your project folder\n",
    "if not os.path.exists(\"Movies\\\\\"):\n",
    "    os.makedirs(\"Movies\\\\\")\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html\"\n",
    "                                                                        #you should change this number\n",
    "page = requests.get(url)    \n",
    "data = page.text\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for link in soup.find_all('a'):\n",
    "    time.sleep(random.randint(1,6)) #waiting a variable amount of seconds for don't be catched by Wikipedia\n",
    "    \n",
    "    localurl= link.get('href')\n",
    "    r = requests.get(localurl) #fetching the html page \n",
    "    \n",
    "    \n",
    "    with open(\"Movies\\\\movie\" + str(counter) +\".html\", 'wb') as f: #creating a new file in the Movies folder \n",
    "        f.write(r.content) #put in this new file the html page of the movie\n",
    "    \n",
    "    counter += 1 #starting form 0 each different movie will take a different index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge scrypt \n",
    "For merge all files in one single folder \n",
    "\n",
    "#### Surce folders: Movies1, Movies2, Movies3\n",
    "#### Destination folder = Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Movies\\\\\"):\n",
    "    os.makedirs(\"Movies\\\\\")\n",
    "\n",
    "start = 0 #starting from file 0\n",
    "for folderNumber in range(1,4):\n",
    "    \n",
    "    fileNumber = 0 #starting from file 0\n",
    "    \n",
    "    while os.path.exists(\"Movies{}\\\\movie{}.html\".format(folderNumber, fileNumber)): #Iterating for each file in the surce\n",
    "        \n",
    "        #pushing the file in the Movies folder\n",
    "        os.rename(\"Movies{}\\\\movie{}.html\".format(folderNumber, fileNumber), \"Movies\\\\movie{}.html\".format(fileNumber + start))\n",
    "        \n",
    "        fileNumber += 1 #mooving to the next file\n",
    "    \n",
    "    start += fileNumber # for the next file we will stars where we left. \n",
    "\n",
    "print(\"moved \", start, \" flies in Movies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from the files and witing the the others\n",
    "This code transforms html files to tsv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNumber = 0 #starting from file 0\n",
    "while os.path.exists(\"Movies\\\\movie\" + str(fileNumber) + \".html\"): #Iterating for each file \n",
    "    \n",
    "    #Parsing the html file:\n",
    "    with open(\"Movies\\\\movie\" + str(fileNumber) + \".html\", 'rb') as html:\n",
    "        soup = BeautifulSoup(html)\n",
    "    \n",
    "    #initialysing variables as NA, so if the sections are missing the variable remains NA \n",
    "    title = \"NA\"\n",
    "    intro = \"NA\"\n",
    "    plot = \"NA\"\n",
    "    link = \"NA\"\n",
    "    infobox = OrderedDict() #we must put as key of the dict the keyword that we will find in the infobox of wikipedia:\n",
    "    infobox[\"name\"] = \"NA\"\n",
    "    infobox[\"Directed by\"] = \"NA\"\n",
    "    infobox[\"Produced by\"] = \"NA\"\n",
    "    infobox[\"Written by\"] = \"NA\"\n",
    "    infobox[\"Starring\"] = \"NA\"\n",
    "    infobox[\"Music by\"] = \"NA\"\n",
    "    infobox[\"Release date\"] = \"NA\"\n",
    "    infobox[\"Running time\"] = \"NA\"\n",
    "    infobox[\"Country\"] = \"NA\"\n",
    "    infobox[\"Language\"] = \"NA\"\n",
    "    infobox[\"Budget\"] = \"NA\"\n",
    "    \n",
    "    # we do fetch the infos only if the page is not a disambiguation page \n",
    "    if not soup.body.find('a', title=\"Help:Disambiguation\"):\n",
    "        #-------------------------- Title, Intro and Plot code ----------------------------------------\n",
    "        title = soup.title.string.strip(\" - Wikipedia\") #taking the title, without the \" - Wikipedia\" last part.\n",
    "\n",
    "        body = soup.body #put the soup body in a variable.\n",
    "        paragraphNumber = 1 #a counter for know whitch paragraf are we working on \n",
    "        for paragraph in body.find_all('p'): #iterating on the paragrafs \n",
    "            if(paragraphNumber == 1):#putting the first paragraf in the intro variable\n",
    "                intro = paragraph.text.strip() #using strip to delete space characters\n",
    "                paragraphNumber += 1\n",
    "            elif(paragraphNumber == 2):#putting the second paragraf in the plot variable\n",
    "                plot = paragraph.text.strip() #using strip to delete space characters\n",
    "                break #we don't need the other paragrafs\n",
    "\n",
    "        link = link = soup.find('link', rel='canonical')[\"href\"]\n",
    "        #-------------------------- Title, Intro and Plot code ----------------------------------------\n",
    "\n",
    "\n",
    "        #-------------------------- Infobox code ----------------------------------------\n",
    "        table = soup.find('table', class_='infobox vevent')\n",
    "\n",
    "        try:\n",
    "            infobox[\"name\"] = table.find('tr').text #let's search the infobox\n",
    "            for tr in table.find_all('tr'): #for each content in the infobox\n",
    "                for section in infobox.keys(): #check for each content that we need...\n",
    "                    try: \n",
    "                        if section == tr.find('th').text: #check if the current tr is the section that we wont\n",
    "                            infobox[section] = tr.find('td').text.strip() #if it is, save it in the infobox[section]\n",
    "                    except: pass #if it is an empty tag\n",
    "        except: pass #if the infobox is not in the page \n",
    "        #-------------------------- Infobox code ----------------------------------------\n",
    "\n",
    "    \n",
    "    #-------------------------- Writing on the file ----------------------------------------\n",
    "    #this lines create a new directory, Movies, in your project folder\n",
    "    if not os.path.exists(\"MoviesTSV\\\\\"):\n",
    "        os.makedirs(\"MoviesTSV\\\\\")\n",
    "    \n",
    "    #writing the results in the tsv file \n",
    "    with open('MoviesTSV\\\\article_' + str(fileNumber) + '.tsv', 'wt', encoding = 'utf-8') as out_file: #creating one file for each different movie \n",
    "        tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([title, intro, plot, link] + list(infobox.values())) #input to the tsv file.\n",
    "    #-------------------------- Writing on the file ----------------------------------------\n",
    "    \n",
    "    fileNumber += 1 #let's move to the next file!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
